# Measures of spread

We saw in the measures of central tendency chapter that the mean can be a poor representation of data if the data is skewed, and that we should therefore be careful when someone presents us with a mean (or average) without any further information.

One of the types of 'further information' that can help us is a measure of spread of the data around the mean value.
We usually use the *variance* and the *standard deviation* to quantify measure of spread.
Both are easy to calculate, and even easier to convert between each other.


## Variance

Lets recap: this is what the income data looks like:

```{r income}
dat %>% 
  unlist() %>% 
  as.vector()
```

To calculate the variance:

1. subtract the mean from each score
1. square the result
1. sum the results to produce one value
1. divide by $n - 1$ (number of observations minus one)

$$
(\#eq:variance)
\frac{\Sigma (x - \bar{x}) ^ 2}{N-1}
$$

Using $n - 1$ rather than simply the number of observations is known as [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction).
To calculate the variance of the population we must assume that the population mean is the same as the sample mean that we have observed.
In fixing the population mean we reduce the degrees of freedom of our observations, because if we change these the final observation is determined in order for the mean to remain constant.

For example if our sample mean is 100 we assume our population mean is 100.
If we have two observations these might be 110 and 90 (mean 100).
If we change the 110 value to 120, the 90 value *must* change to 80 to ensure the sample mean (and therefore the population mean) remains 100, so there is only one degree of freedom.
We would therefore use $2 - 1$ as the denominator in our variance calculation.

```{r income-variance}
var(dat$income)
```


## Standard deviation

As you've probably noticed the variance is not in the units of the original data (otherwise the variance would be Â£`r var(dat$income)`).
This is where the standard deviation comes in.
In fact the unit of the variance is the *square* of the unit of the original data.
The standard deviation is therefore a measure of spread in the unit of the original data, and is calculated simply by square rooting the variance.

$$
(\#eq:standard-deviation)
\sqrt{\frac{\Sigma (x - \bar{x}) ^ 2}{N-1}}
$$

The standard deviation of the income is therefore:

```{r income-sd}
sd(dat$income)
```

The standard deviation is a measure of how far the data points are on average from the mean.
A small standard deviation means the mean fairly accurately represents the data; a large standard deviation means the mean does not represent the data well.


## Standard error and confidence intervals

Standard error is effectively the standard deviation of the population mean.
The standard deviation quantifies how well the sample mean fits the observed (i.e. sample) data, not the population, but we're really interested in how well our sample mean represents the population.

Because any sample we take from the population is going to be slightly different from all other samples (because everything varies) each sample mean is going to be slightly different from every other.
The standard error of the mean is a measure of how confident our sample mean matches the population mean.

One approach to calculate the standard error of the mean would be to take multiple samples.
The mean of each of these samples would form a sampling distribution due to variation: some sample means would be lower than the population mean; some sample means would be higher than the population mean; and many would be the same.
These sample mean values would form a normal distribution around the population mean.

The standard deviation of these sample means would tell us how well our sample means fit the population mean.
This is known as the standard error of the mean ($SE_{\bar{x}}$).

In practice we can usually only take one sample so we can estimate it with:

$$
(\#eq:standard-error)
\sigma_{\bar{x}} = \frac{s}{\sqrt{n}}
$$

where $\sigma_{\bar{x}}$ is the standard error of the population mean, $s$ is the sample standard deviation, and $n$ is the number of observations in the sample.

We can demonstrate this with the census (in fact, we could demonstrate this with any data set and pretend it's the population and take multiple samples from it, but why not just use an actual population?).
I'm using ages of all people in Sheffield in 2011 to illustrate this, which I download from Nomisweb:

```{r download-sheffield-age, cache=TRUE, message=FALSE}
tempdir = tempdir()
download.file(
  "https://www.nomisweb.co.uk/api/v01/dataset/NM_503_1.data.csv?date=latest&geography=1946157123&rural_urban=0&c_age=1...101&measures=20100&signature=NPK-0c73734c0f725c979cee3a:0xa9b892a105be9e9449cdb6c88bdac678e12b229e",
  destfile = paste0(tempdir, "census.csv")
)

age = readr::read_csv(paste0(tempdir, "census.csv"))

age =
  age %>% 
  select(C_AGE_NAME, OBS_VALUE) %>% 
  filter(C_AGE_NAME != "Age 100 and over") %>% 
  mutate(
    C_AGE_NAME = if_else(C_AGE_NAME == "Age under 1", "0", C_AGE_NAME)
  ) %>% 
  mutate(C_AGE_NAME = str_replace(C_AGE_NAME, "Age ", "")) %>% 
  mutate(C_AGE_NAME = as.integer(C_AGE_NAME)) %>% 
  uncount(OBS_VALUE)
```

Figure \@ref(fig:age-hist) is a histogram of ages in people in Sheffield from the 2011 Census.

```{r age-hist, fig.height=3.5, fig.cap="Histogram of ages in Sheffield", cache=TRUE}
ggplot(age) + geom_histogram(aes(C_AGE_NAME), binwidth = 1) +
  xlab("Age") + ylab("Frequency")
```

```{r mode-age}
mode_age =
  age %>% 
  count(C_AGE_NAME) %>% 
  arrange(desc(n)) %>% 
  filter(row_number() == 1) %>% 
  select(C_AGE_NAME) %>% 
  unlist()
```

The modal age is `r mode_age`; the median age is `r median(age$C_AGE_NAME)`; and crucially the mean age is `r mean(age$C_AGE_NAME)`.

Let's take 1,000 samples from the population, and make a sampling distribution of these means:

```{r sample-age-1000}
set.seed(42)
samples = replicate(1000, sample_n(age, 1000))
samples = map(samples, mean)
samples = do.call(rbind, samples)
colnames(samples) = "mean_age"
samples = as.data.frame(samples)
```

```{r samples-age-hist, cache=TRUE, fig.cap="Histogram of samples from age data set", fig.height=3.5}
ggplot(samples) + geom_histogram(aes(mean_age), bins = 50) +
  xlab("Sample mean age") + ylab("Frequency")
```

From Figure \@ref(fig:samples-age-hist) most sample means are around 38, although a few are as low as 36 and as high as 40.
Remember we know are population mean is `r mean(age$C_AGE_NAME)` but we wouldn't normally know this.
If we just had access to one sample, how would we know if the resultant sample mean was close to the population mean?
From the histogram of sample means we can see that it's more likely to end up with a sample mean that's close to the population mean than one that's incorrect, and we can quantify this with a confidence interval.

Let's take a sample of 1000 random cases from this data set and pretend it's all we have access to:

```{r age-sample}
age_sample =
  age %>% 
  sample_n(1000)
```

The mean of this sample is `r mean(age_sample$C_AGE_NAME)`, very close to the population mean but not quite the same.
The standard error of this sample is:

```{r standard-error-sample-age}
se_age = sd(age_sample$C_AGE_NAME) / sqrt(nrow(age_sample))
se_age
```

We know that an ideal normal distribution will have [95\% of cases within 1.96 standard deviations of the mean](https://en.wikipedia.org/wiki/Normal_distribution#Standard_deviation_and_coverage).
If we multiply our standard error by $\pm$ 1.96 that therefore defines a 95\% confidence interval.
In this case `r mean(age_sample$C_AGE_NAME) - (1.96 * se_age)` to `r mean(age_sample$C_AGE_NAME) + (1.96 * se_age)`.
This effectively estimates that if we were to take 100 samples the population mean would fall within these bounds 95 times.
In our example the population mean is indeed within the 95\% confidence interval of the sample mean.

To calculate 99\% confidence intervals use 2.58 standard deviations rather than 1.96, in this example resulting in a confidence interval between `r mean(age_sample$C_AGE_NAME) - (2.58 * se_age)` and `r mean(age_sample$C_AGE_NAME) + (2.58 * se_age)`
Perhaps counterintuitively this results in a wider interval (because the interval ensures the population mean falls within these bounds 99 times instead of 95); it is not more precise.

From the standard error of the mean and confidence interval we can therefore quantify how confident we are that the sample mean is close to the true population mean.
