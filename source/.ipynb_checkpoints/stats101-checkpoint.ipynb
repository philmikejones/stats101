{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard error and confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"../data/external/age.csv\"):\n",
    "    age = \"https://www.nomisweb.co.uk/api/v01/dataset/NM_503_1.data.csv?date=latest&geography=1946157123&rural_urban=0&c_age=1...101&measures=20100&signature=NPK-0c73734c0f725c979cee3a:0xa9b892a105be9e9449cdb6c88bdac678e12b229e\"\n",
    "    age = requests.get(age)\n",
    "    age = age.text\n",
    "    outfile = open(\"../data/external/age.csv\", \"w\")\n",
    "    outfile.write(age)\n",
    "    \n",
    "age = pd.read_csv(\"../data/external/age.csv\")\n",
    "age = age[[\"C_AGE_NAME\", \"OBS_VALUE\"]]\n",
    "age.C_AGE_NAME = age.C_AGE_NAME.str.replace(\"Age under 1\", \"0\")\n",
    "age.C_AGE_NAME = age.C_AGE_NAME.str.replace(\" and over\", \"\")\n",
    "age.C_AGE_NAME = age.C_AGE_NAME.str.replace(\"Age \", \"\")\n",
    "age.C_AGE_NAME = pd.to_numeric(age.C_AGE_NAME)\n",
    "sum_obsvalue = age.OBS_VALUE.sum()  # for checks\n",
    "\n",
    "age = age.loc[age.index.repeat(age[\"OBS_VALUE\"])]\n",
    "\n",
    "if len(age.index) != sum_obsvalue:\n",
    "    raise Exception(\"age data frame not spread correctly\")\n",
    "\n",
    "age = age.drop(\"OBS_VALUE\", axis = 1)  # default 'axis' is 'index'\n",
    "    \n",
    "age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"../data/external/age_eb.csv\"):\n",
    "    age_eb = \"https://www.nomisweb.co.uk/api/v01/dataset/NM_1531_1.data.csv?date=latest&geography=1946157295&c_age=1...101&measures=20100&signature=NPK-0c73734c0f725c979cee3a:0x65c03934cecfca562500f65e9cb1bd2083fbea01\"\n",
    "    age_eb = requests.get(age_eb)\n",
    "    age_eb = age_eb.text\n",
    "    outfile = open(\"../data/external/age_eb.csv\", \"w\")\n",
    "    outfile.write(age_eb)\n",
    "    \n",
    "age_eb = pd.read_csv(\"../data/external/age_eb.csv\")\n",
    "age_eb = age_eb[[\"C_AGE_NAME\", \"OBS_VALUE\"]]\n",
    "age_eb.C_AGE_NAME = age_eb.C_AGE_NAME.str.replace(\"Age under 1\", \"0\")\n",
    "age_eb.C_AGE_NAME = age_eb.C_AGE_NAME.str.replace(\" and over\", \"\")\n",
    "age_eb.C_AGE_NAME = age_eb.C_AGE_NAME.str.replace(\"Age \", \"\")\n",
    "age_eb.C_AGE_NAME = pd.to_numeric(age_eb.C_AGE_NAME)\n",
    "sum_obsvalue = age_eb.OBS_VALUE.sum()  # for checks\n",
    "\n",
    "age_eb = age_eb.loc[age_eb.index.repeat(age_eb[\"OBS_VALUE\"])]\n",
    "\n",
    "if len(age_eb.index) != sum_obsvalue:\n",
    "    raise Exception(\"age_eb data frame not spread correctly\")\n",
    "\n",
    "age_eb = age_eb.drop(\"OBS_VALUE\", axis = 1)\n",
    "\n",
    "age_eb.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can calculate a standard error of many parameters, but typically it refers to the standard error of the mean ($SE_{\\bar{x}}$).\n",
    "The standard error of the mean differs from the standard deviation: the standard deviation quantifies how well the sample mean fits the observed (i.e. sample) data, and the standard error of the mean quantifies how well the sample mean matches the population mean.\n",
    "\n",
    "Because any sample we take from the population is going to be slightly different from all other samples (because everything varies) each sample mean is going to be slightly different from every other.\n",
    "The standard error of the mean is a measure of how confident our sample mean matches the population mean.\n",
    "\n",
    "One approach to calculate the standard error of the mean would be to take multiple samples.\n",
    "The mean of each of these samples would form a sampling distribution due to variation: some sample means would be lower than the population mean; some sample means would be higher than the population mean; and many would be the same.\n",
    "These sample mean values would form a normal distribution around the population mean.\n",
    "The standard deviation of these sample means would tell us how well our sample means fit the population mean.\n",
    "\n",
    "In practice we can usually only take one sample so we can estimate it with:\n",
    "\n",
    "$$\n",
    "\\sigma_{\\bar{x}} \\approx \\frac{s}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "where $\\sigma_{\\bar{x}}$ is the standard error of the population mean (the parameter we're trying to estimate), $s$ is the sample standard deviation, and $n$ is the number of observations in the sample ($\\approx$ just means 'approximately equal to').\n",
    "\n",
    "We can demonstrate this with the census (in fact, we could demonstrate this with any data set and pretend it's the population and take multiple samples from it, but why not just use an actual population?).\n",
    "I'm using ages of all people in Sheffield in 2011 to illustrate this, which I download from Nomisweb:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a histogram of ages of people living in Sheffield from the 2011 Census."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age.hist(\"C_AGE_NAME\", bins = 100)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median age is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age.C_AGE_NAME.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean age is (remember this is the mean of the population, which we wouldn't normally know):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age.C_AGE_NAME.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take 1000 samples of 100 people from the population, and make a sampling distribution of these means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for i in range(1000):\n",
    "    sample = age.sample(n = 100, replace = True, random_state = i)\n",
    "    sample = sample.C_AGE_NAME.mean()\n",
    "    samples.append(sample)\n",
    "\n",
    "samples = pd.Series(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a distribution of the sample means (a sampling distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.hist(bins = 20)\n",
    "plt.xlabel(\"Mean age\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sample means are around 38, although a few are lower than 32 and higher than 45. Remember  in this case we know the population mean, but we wouldn't normally know this.\n",
    "If we just had access to one sample, how would we know if the resultant sample mean was close to the population mean?\n",
    "\n",
    "From the histogram of sample means we can see that it's more likely to end up with a sample mean that's close to the population mean than one that's further away, and we can quantify this with a confidence interval.\n",
    "\n",
    "Let's take one sample of 1000 random cases from the original data set and pretend it's all we have access to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = age.sample(n = 1000, replace = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of this sample is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.C_AGE_NAME.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very close to the population mean but not quite the same.\n",
    "The standard error of this sample mean is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.C_AGE_NAME.sem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that an ideal normal distribution will have [95\\% of cases within 1.96 standard deviations of the mean](https://en.wikipedia.org/wiki/Normal_distribution#Standard_deviation_and_coverage).\n",
    "If we multiply our standard error by $\\pm$ 1.96 that therefore defines a 95% confidence interval.\n",
    "In this case we would have an interval of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.C_AGE_NAME.mean() - 1.96 * sample.C_AGE_NAME.sem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.C_AGE_NAME.mean() + 1.96 * sample.C_AGE_NAME.sem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This effectively estimates that if we were to take 100 samples the population mean would fall within these bounds 95 times.\n",
    "In our example the population mean is indeed within the 95% confidence interval of the sample mean (remember the population mean was:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age.C_AGE_NAME.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate 99% confidence intervals use 2.58 standard deviations rather than 1.96, in this example resulting in a confidence interval between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.C_AGE_NAME.mean() - 2.58 * sample.C_AGE_NAME.sem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.C_AGE_NAME.mean() + 2.58 * sample.C_AGE_NAME.sem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps counterintuitively this results in a wider interval (because the interval ensures the population mean falls within these bounds 99 times instead of 95); it is not 'more precise'.\n",
    "\n",
    "From the standard error of the mean and confidence interval we can therefore quantify how confident we are that the sample mean is close to the true population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing means and confidence intervals\n",
    "\n",
    "A useful property of confidence intervals is that they can be used to compare two or more means to see if they are statistically significantly different.\n",
    "For example, we have a sample of the ages of people in Sheffield and we could create a similar sample of the ages of people in Eastbourne, calculate the confidence intervals, and compare them to see if they differ (i.e. if, on average, people are older in Eastbourne)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above we can see that the mean *is* higher than the mean for Sheffield, but remember we are comparing populations and we would not typically have access to this information.\n",
    "Let's take a sample of 1,000 individuals and calculate the mean and standard error of the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_eb = age_eb.sample(n = 1000, replace = True, random_state = 42)\n",
    "sample_eb.C_AGE_NAME.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our sample mean is also different, but can we be sure it's different and not just the result of our sampling?'. Using the standard error of the mean our confidence interval is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_eb.C_AGE_NAME.mean() - 1.96 * sample_eb.C_AGE_NAME.sem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_eb.C_AGE_NAME.mean() + 1.96 * sample_eb.C_AGE_NAME.sem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, reassuringly, this interval contains our population mean. Note also that the full interval is higher than the Sheffield interval (i.e. the top of the Sheffield interval is below the bottom of the Eastbourne interval). This means that the mean age of Sheffield and Eastbourne are statistically significantly different at the 95% confidence level.\n",
    "\n",
    "We can visualise the confidence intervals as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cis = {\"town\": [\"Sheffield\", \"Eastbourne\"],\n",
    "           \"mean\": [sample.C_AGE_NAME.mean(), sample_eb.C_AGE_NAME.mean()],\n",
    "           \"sem\":  [sample.C_AGE_NAME.sem(), sample_eb.C_AGE_NAME.sem()]}\n",
    "\n",
    "age_cis = pd.DataFrame.from_dict(age_cis)\n",
    "age_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(\n",
    "    x = [1, 2], y = age_cis[\"mean\"],\n",
    "    xerr = 0.02, yerr = 1.96 * age_cis[\"sem\"],\n",
    "    linestyle = \"\"\n",
    ")\n",
    "plt.xticks([1, 2], age_cis[\"town\"])\n",
    "plt.ylabel(\"Mean age\")\n",
    "plt.xlabel(\"Town\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal distribution\n",
    "\n",
    "The normal distribution is central to our ability to infer about a population from a sample. The normal distribution looks like this (by Dan Kernler from Wikimedia Commons, CC BY-SA 4.0):\n",
    "\n",
    "<a title=\"By Dan Kernler [CC BY-SA 4.0 \n",
    " (https://creativecommons.org/licenses/by-sa/4.0\n",
    ")], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Empirical_Rule.PNG\"><img width=\"512\" alt=\"Empirical Rule\" src=\"https://upload.wikimedia.org/wikipedia/commons/a/a9/Empirical_Rule.PNG\"></a>\n",
    "\n",
    "The normal distribution was discovered by Gauss (which is why it's also sometimes called the Gaussian distribution) and described an 'ideal' situation. Lots of data follow this pattern: the height or weight of a population;  The mean of the normal distribution ($\\mu$ on the diagram above) is the centre. Because the normal distribution is **symmetrical** we know that 50% of cases fall below and 50% of cases fall above the mean.\n",
    "\n",
    "Another useful property of the normal distribution is we know, or can calculate, how many cases fall with 1, 2, 3, or more standard deviations of the mean (these are shown as $\\mu \\pm \\sigma; \\mu \\pm 2\\sigma; \\mu \\pm 3\\sigma$ on the figure). These are about 68%, 95%, and 97.5% respectively.\n",
    "\n",
    "|Number of $\\sigma$ from mean | Percent of cases|\n",
    "|-----------------------------|-----------------|\n",
    "| 1                           | 68.27%          |\n",
    "| 1.96                        | 95%             |\n",
    "| 2                           | 95.45%          |\n",
    "| 2.58                        | 99%             |\n",
    "| 3                           | 99.73%          |\n",
    "\n",
    "Therefore if we know the mean ($\\mu$) and standard deviation ($\\sigma$) of our sample we can calculate the confidence interval of our sample mean (as we did above). Typically we calculate a 95% confidence interval (although 99% is also common), and this tells us the likely range the population mean falls within. This is why we used the figure 1.96 when calculating our confidence interval earlier, and this is the property that allows us to infer information about the population from our smaller sample.\n",
    "\n",
    "The other, related, way we can use this information is if we have a mean and standard deviation and observe a data point, we can calculate how many standard deviations from the mean this data point is. We can then see if the observed data point falls within the normal variation we expect (i.e. within 1.96 standard deviations for 95% confidence) or outside it, and is therefore the result of something not within the normal range.\n",
    "\n",
    "Remember our household weekly income example, after we removed the top--coded responses? It looked something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_trimmed.P344pr.hist(bins = 100)\n",
    "plt.xlabel(\"Weekly income (£)\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean income is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_trimmed.P344pr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the standard deviation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_trimmed.P344pr.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's imagine we find a respondent who lives in Chelsea and we ask them to complete our survey. They respond that their income is £2,000 per week. Does this fall within the variability we expect, or is it statistically significantly likely that this respondent has a higher income than most? Well, we can calculate how many standard deviations our observed data point is away from the mean. We know:\n",
    "\n",
    "$$\n",
    "2000 = \\mu + x.\\sigma\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean, $\\sigma$ is the standard deviation, and $x$ is the number of standard deviations we want to calculate. If we rearrange the equation we get:\n",
    "\n",
    "$$\n",
    "\\frac{2000}{\\sigma} = \\frac{\\mu}{\\sigma} + x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{(2000 - \\mu)}{\\sigma} = x\n",
    "$$\n",
    "\n",
    "Plugging in the mean and standard deviation we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2000 - food_trimmed.P344pr.mean()) / food_trimmed.P344pr.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our observed data point is more than five standard deviations higher than the mean, which means that if we were to interview 3.5 million people, only one would have an income that high. It's therefore highly likely that this respondent has a higher income than the average. In physics this would be known as a 'five-sigma' result: i.e. the result is more than five standard deviations ($\\sigma$) from the mean and is therefore highly unlikely to be through chance alone (in the social sciences we usually opt for the '1.96 sigma' rule).\n",
    "\n",
    "Not all observed data form a perfect normal distribution (in fact most differ at least slightly). There are two ways we need to describe a distribution if it is different from the normal: skewness and kurtosis.\n",
    "\n",
    "## Skewness\n",
    "\n",
    "A normal distribution has its mean, median, and mode at the same point (the centre). Skewness means the data points are skewed one way or the other:\n",
    "\n",
    "<a title=\"By Rodolfo Hermans (Godot) at en.wikipedia. [CC BY-SA 3.0 \n",
    " (https://creativecommons.org/licenses/by-sa/3.0\n",
    ")], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Negative_and_positive_skew_diagrams_(English).svg\"><img width=\"500\" alt=\"Negative and positive skew diagrams (English)\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Negative_and_positive_skew_diagrams_%28English%29.svg/500px-Negative_and_positive_skew_diagrams_%28English%29.svg.png\"></a>\n",
    "\n",
    "Negative skew means the tail is to the left; positive skew means the tail is to the right. In a positively skewed distribution the mode and median are lower than the mean. In a negatively skewed distribution the median and mode are higher than the mean.\n",
    "\n",
    "## Kurtosis\n",
    "\n",
    "Kurtosis refers to how bunched (clustered) around the mean the data points are. Positive kurtosis (leptokurtic) means the points are clustered around the mean, making the distribution narrower and taller than a normal distribution. Negative kurtosis (platykurtic) means the data points are spread out more widely, resulting in a distribution that is flatter and broader than a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "variance = 1\n",
    "sigma = math.sqrt(variance)\n",
    "normal = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 100)  # probability density func\n",
    "plt.plot(normal, scipy.stats.norm.pdf(normal, mu, sigma))\n",
    "\n",
    "variance = 2\n",
    "sigma = math.sqrt(variance)\n",
    "platykurtic = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 100)\n",
    "plt.plot(platykurtic, scipy.stats.norm.pdf(platykurtic, mu, sigma))\n",
    "\n",
    "variance = 0.5\n",
    "sigma = math.sqrt(variance)\n",
    "leptokurtic = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 100)\n",
    "plt.plot(leptokurtic, scipy.stats.norm.pdf(leptokurtic, mu, sigma))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above:\n",
    "\n",
    "- the **blue** line is a normal distribution,\n",
    "- the **green** line is a distribution with positive kurtosis (leptokurtic)\n",
    "- the **orange** line is a distribution with negative kurtosis (platykurtic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing\n",
    "\n",
    "So far we've loaded our data set, described it with measures of central tendency and variability, and tested to see if our sample mean adequately describes our population mean.\n",
    "Now we move on to testing hypotheses.\n",
    "\n",
    "A hypothesis is a statement that we make to explain a phenomenon that we do not yet know the answer to.\n",
    "A hypothesis must be testable.\n",
    "\n",
    "For example, in our data set we have two nominal variables that we might propose there is a relationship between:\n",
    "\n",
    "- NS-SEC class of the household reference person (`A094r`)\n",
    "- Tenure (`A121r`)\n",
    "\n",
    "NS-SEC stands for '[National Statistics Socio--economic classification](https://www.ons.gov.uk/methodology/classificationsandstandards/otherclassifications/thenationalstatisticssocioeconomicclassificationnssecrebasedonsoc2010)', and is a measure of employment grade, for example if the household reference person is a higher manager or professional, or a manual worker.\n",
    "\n",
    "The [household reference person](https://www.scotlandscensus.gov.uk/variables-classification/household-reference-person) is the person in the household (usually a family) who is full--time employed or, if both partners are full--time employed, the one who is oldest.\n",
    "This concept is used because families share social, cultural, and economic characteristics so, for example, if one partner is currently unemployed they share some of their characteristics with the HRP (for example they are likely to still live in the family home and participate in similar activities).\n",
    "Similarly, children who do not yet work can be ascribed economic and social characteristics based on their parent or carer's economic activity.\n",
    "\n",
    "Tenure simply means if the respondent owns their home (outright, or with a mortgage) or rents their home from a private landlord or council.\n",
    "\n",
    "With all this in mind, our hypothesis might be:\n",
    "\n",
    "> There is an association or link between household reference person NS-SEC and home ownership (tenure).\n",
    "\n",
    "For example, people in NS-SEC category 1 (higher managerial, administrative, and professional occupations) may be more likely to own their home compared to people in routine and manual occupations.\n",
    "\n",
    "First of all, let's look at a crosstabulation (crosstab) of frequencies comparing these two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index = food.A094r, columns = food.A121r, margins = True, margins_name = \"Total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this crosstab we can see that similar numbers of people rent their homes from local authorities ('public rented') and private landlords (880 and 798 respectively), but that more than four times as many people own their own home (3466) than rent privately or rent publicly.\n",
    "\n",
    "We can also see that most people in NS-SEC group 1 (higher managerial, administrative, and professional occupations) own their home (1282) compared to rent (total 307).\n",
    "If we compare this to NS-SEC group 3 (routine and manual occupations) only 523 own their home, while 289 rent from a local authority (much more than the group 1) and 233 rent privately (similar to group 1).\n",
    "\n",
    "These descriptions are pretty straightforward, but any analysis is complicated by the fact that the two groups are different sizes (n = 1,589 group 1; n = 1,045 group 3) so we cannot directly compare the counts in this table to see if there are differences between the groups.\n",
    "The next step is to look at the percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    index = food.A094r, columns = food.A121r,\n",
    "    normalize = \"index\"\n",
    ") * 100  # converts proportions to percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the row percentages (i.e. each row adds up to 100%) we can see that approximately 80% of higher managerial and professional families own their own home, but only 50% of routine and manual families own their own homes.\n",
    "Similarly we can see that only about 4.5% of managerial and professional families rent from a local authority, but 27% of routine and manual families do (remember rows 4 and 5 are unemployed and unclassified respectively).\n",
    "\n",
    "So we think there's an association between NS-SEC of the household reference person and tenure, and the crosstabs certainly seem to support this.\n",
    "Unfortunately humans are very, very good at spotting patterns, even when there isn't one there, so instead of just relying on our say--so we can statistically test to see if there really is a difference.\n",
    "To do this we use a hypothesis test, of which the most common type is the chi--squared ($\\chi ^ 2$) test (the Greek letter is pronounced 'key', but 'kai' is probably more common).\n",
    "\n",
    "To perform a chi--squared test we specify a *null hypothesis*, which we denote $H_0$.\n",
    "A null hypothesis is a way of framing our hypothesis that (usually) states there is *no* association between our variables, so in our case we specify our null hypothesis as:\n",
    "\n",
    "> There is *no* association between NS-SEC of the household reference person and housing tenure\n",
    "\n",
    "The opposite of the null hypothesis is the *alternative hypothesis*, $H_1$, which is usually our original hypothesis.\n",
    "\n",
    "It is important to frame a hypothesis test in this way because we assume the absence of an association, and it is up to us as researchers to provide evidence that there is an association.\n",
    "For example, we cannot assume that people who drink coffee are more intelligent than people who drink tea.\n",
    "It is up to us to demonstrate that this is the case.\n",
    "This is what makes our hypothesis *testable* and *falsifiable*.\n",
    "\n",
    "It's a bit like the presumption of innocence: we cannot be locked up unless we are proven to be guilty of a crime.\n",
    "If it were the other way around (i.e. presumption of guilt) we would all be incarcerated and we would have to prove that we were innocent, not just of one crime, but of every conveivable crime in order to be released!\n",
    "This would be an impossible task (not least because no doubt someone would add another charge arbitrarily).\n",
    "\n",
    "Hermione Granger sums this up better than most statistics textbooks ever did:\n",
    "\n",
    "> \"Well, how can that `[the resurrection stone]` be real?\"  \n",
    "> \"Prove that it is not\", said Xenophilius.  \n",
    "> Hermione looked outraged.  \n",
    "> \"But that's --- I'm sorry, but that's completely ridiculous! How can I possibly prove it doesn't exist? Do you expect me to get hold of --- of all the pebbles in the world, and test them? I mean, you could claim that anything's real if the only basis for believing in it is that nobody's *proved* it doesn't exist!  \n",
    "> - Harry Potter and the Deathly Hallows\n",
    "\n",
    "To carry out the $\\chi ^ 2$ test, the [`scipy.stats.chi2_contingency()`](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.chi2_contingency.html) function returns the following pieces of information:\n",
    "\n",
    "- The test statistic\n",
    "- The $p$ value\n",
    "- The number of [degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics))\n",
    "- The expected counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.chi2_contingency(\n",
    "    pd.crosstab(index = food.A094r, columns = food.A121r, margins = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test statistic is, roughly, the amount of variance explained by our test compared to the amount of variance not explained.\n",
    "In all my years of statistics I have never worked one of these out by hand, so don't worry too much about this.\n",
    "\n",
    "The degrees of freedom are the the number of independent pieces of information to perform the test on (a bit like we saw earlier with the standard deviation, the DOF used is $n - 1$ because we set the population mean to be the sample mean).\n",
    "In a cross tab this is the number of rows minus 1, multiplied by the number of columns minus 1, in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(5 - 1) * (3 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because, in this example, once we know rows 1--4 we can calculate row 5 because we know the total.\n",
    "Similarly once we know columns 1--2 we can calculate column 3 because we know the total.\n",
    "\n",
    "We're not interested in the test statistic or degrees of freedom directly, but these are used to calculate the $p$ value.\n",
    "We want the $p$ value to be low, by convention at least below 0.05.\n",
    "\n",
    "In this case the $p$ value is so low it is returned in scientific notation.\n",
    "The `3.2e-144` means the decimal place is moved 144 places to the left, i.e.:\n",
    "\n",
    "`0.0000000000000000000000000000000000000000000000000000000000000000000000000000\n",
    "000000000000000000000000000000000000000000000000000000000000000000003`\n",
    "\n",
    "So, essentially, zero (in fact it's *highly* dubious that the p value is known to this level of accuracy, so we treat it as essentially zero).\n",
    "A $p$ value this small means it is very, very unlikely that we would have observed the relationship we did just by chance, so we can say with some confidence that there is an association or link between NS-SEC and housing tenure.\n",
    "\n",
    "There are a few important assumptions we must satisfy to use a chi--squared test.\n",
    "One of these is to do with *expected frequencies*, which are used in calculating the actual chi--squared statistic.\n",
    "In calculating the chi--squared statistic we calculate the expected frequency for each cell.\n",
    "In our example we have 15 cells in our crosstab, so we calculate 15 expected frequencies.\n",
    "\n",
    "Specifically we should not have any expected frequencies of 0 (should be at least 1), and no more than 20% of expected frequencies should be less than 5.\n",
    "To calculate the expected frequency for each cell we use the formula:\n",
    "\n",
    "$$\n",
    "E_{ij} = \\frac{T_{i} x T_{j}}{N}\n",
    "$$\n",
    "\n",
    "where $E_{ij}$ is the expected frequency of cell in row $_i$ and column $_j$; $T_i$ is the total of row $_i$; $T_j$ is the total of column $_j$; and $N$ is the table grand total.\n",
    "So the expected frequency for row 1, column 1 is:\n",
    "\n",
    "$$\n",
    "E_{1, 1} = \\frac{1589 x 880}{5144}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1589 * 880) / 5144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is what is returned by the `scipy.stats.chi_contingency()` function.\n",
    "\n",
    "When running a chi--square test on a 2x2 contingency table it is likely to produce p values that are too small (i.e. it's more likely to make a false positive or a [type I error](#interpreting-the-results).\n",
    "To correct this `scipy.stats.chi_contingency()` automatically applies the [**Yates's continuity correction**](https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity) if you're performing a test on a 2x2 table.\n",
    "I've never worried about what this is or how it works (although Andy Field's textbook, as usual, covers it in an accessible way); just know that it has been applied when reporting on a 2x2 table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "\n",
    "There are two problems to be aware of when interpreting the results of a hypothesis test.\n",
    "The hypothesis test does not *prove* an association between our variables; it gives us a statistical level of confidence that there is an association.\n",
    "\n",
    "There is always a risk that we might reject the null hypothesis (i.e. state that there is an association) when there isn't one.\n",
    "In our example we have good statistical evidence that there is an association (the $p$ value is very low), but there is still a possibility that this could have simply happened randomly (admittedly a very small chance).\n",
    "\n",
    "If this were a random occurrence but we stated there was an association this would be called a **Type I (one) error**, sometimes known as a false positive.\n",
    "\n",
    "The other type of error we could make is failing to reject the null hypothesis when we should (i.e. we state there is no association when there *is* one), also known as a false negative.\n",
    "This is known as a **Type II (two) error**.\n",
    "\n",
    "When we perform a hypothesis test we therefore need to balance the risk of stating that there is an association when there isn't, and the risk of stating that there is no association when there is one.\n",
    "\n",
    "Depending on our research question, one or the other errors might be more problematic.\n",
    "For example, if we are testing a new drug we want to make sure it is effective, so we do not want to make a type I error (i.e. state that there is an association when there isn't one).\n",
    "But if we're testing the drug for side effects we want to make sure we don't make a type II error (i.e. assert that there are no side effects when, in fact, there are)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directional tests\n",
    "\n",
    "Two--tailed tests are the default, and what you should use unless you have a very good (and documented!) reason for using a one--tailed test.\n",
    "\n",
    "One--tailed tests are used when we are carrying out a *directional* test.\n",
    "For example, we previously tested if there is an association between employment grade (NS--SEC) and housing tenure.\n",
    "We did not specify a direction, so we would usually use a two--tailed test.\n",
    "However, if we specified our alternative hypothesis with a direction, we would run a one--tailed test.\n",
    "For example, if our alternative hypothesis were:\n",
    "\n",
    "> People of higher employment grade are **more likely** to own their own home\n",
    "\n",
    "we now have a directional test (i.e. we don't think they're less likely to own their own home).\n",
    "\n",
    "In this case we can use a one--tailed test.\n",
    "Note that we have stated our hypothesis **before** we ran the test; you cannot run a one--tailed test after the fact and claim you've found a directional association.\n",
    "Also note that if you do not find a directional association and later want to switch direction you cannot.\n",
    "Therefore one--tailed tests tend to be used when previous literature identifies a directional association and you want to use new data to test it.\n",
    "\n",
    "The reason for this skepticism of one--tailed tests is that they require a smaller difference between the two variables to be statistically significant.\n",
    "If you are performing a non--directional (two--tailed test) at a confidence level of 0.05, your have half of this at each end of your distribution (0.025) to work with to detect a difference.\n",
    "If you specify a directional (i.e. one--tailed test) you have more of the distribution to work with to detect a significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect sizes\n",
    "\n",
    "Determining that there *is* an association is all very well and good, but it tells us nothing of what the size of the effect is.\n",
    "For example, our hypothesis test has determined it is probable that there is an association between the employment grade of the household reference person and tenure, but it does not tell us *how much* more likely they are to own their home.\n",
    "\n",
    "The most common measures of effect size are:\n",
    "\n",
    "- odds ratio\n",
    "- Pearson's correlation coefficient, $r$\n",
    "\n",
    "There are others, for example Cohen's $d$ which is useful if the group sizes are very different, but these are the two most common in the social sciences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds ratio\n",
    "\n",
    "The odds ratio is a way of specifying the effect size for 2x2 contingency tables.\n",
    "For our example of employment grade and housing tenure we have more than a 2x2 table, but we can restate it so instead of just measuring an association between all employment grades and tenure types we can calculate the odds ratio of professional and managerial respondents owning their home and routine and manual respondents owning their home.\n",
    "\n",
    "Here's a reminder of the frequencies of all employment grades and tenure types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index = food.A094r, columns = food.A121r, margins = True, margins_name = \"Total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds ratio is the odds of one group for the event of interest divided by the odds of the other group for the event of interest.\n",
    "So we need two sets of odds.\n",
    "\n",
    "First we specify the odds of the professional and managerial group owning their own home.\n",
    "This is the number of professional respondents who own their home (1282), divided by the number of professional respondents who *do not* own their home (70 + 237):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1282 / (70 + 237)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that, roughly, for every professional and managerial respondent who *does not* own their home there are four who do.\n",
    "\n",
    "Similarly the odds of a routine and manual respondent owning their home is the number of routine and manual respondents who own their home (523) divided by the number of routine and manual respondents who *do not* own their own home (289 + 233):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "523 / (289 + 233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that, roughly, for every routine and manual respondent who *does not* own their home there is one who does, so the odds are about equal or 1:1.\n",
    "\n",
    "The odds ratio is simply one divided by the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1282 / (70 + 237)) / (523 / (289 + 233))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that professional and managerial respondents are 4.168 times more likely to own their home than routine and manual respondents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson's correlation coefficient, $r$\n",
    "\n",
    "$r$ is standardised, which is useful because:\n",
    "\n",
    "- tests of all sorts of units can be compared which each other,\n",
    "- the result is between -1 (perfect negative association), through 0 (no association), and 1 (perfect positive association)\n",
    "\n",
    "$r$ is a measure of effect size (or correlation) between two numerical variables.\n",
    "It works on the principle that as the difference from the mean for one variable increases we expect the difference from the mean for the related variable to increase (positive correlation) or decrease (negative correlation).\n",
    "\n",
    "For example the mean income is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_trimmed.P344pr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we hypothesise that people with higher incomes spend more money on food (they have more money to shop at Waitrose).\n",
    "Expenditure is top--coded, so let's trim the data like we did for income and take a look at the resulting distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_trimmed = food_trimmed[food_trimmed.P550tpr < food_trimmed.P550tpr.max()]\n",
    "food_trimmed.hist(column = \"P550tpr\", bins = 100)\n",
    "plt.xlabel(\"Food expenditure (£)\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean expenditure is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_trimmed.P550tpr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take an individual with a high income (their income deviates from the mean) we would expect their expenditure to also deviate from the expenditure mean.\n",
    "These deviations from the mean are their variances, so we are stating that we expect income and expenditure on food to **covary**.\n",
    "This principle is used to calculate the **Pearson correlation coefficient** (usually just called the correlation), which is a standardised measure of how much the two variables vary together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(\n",
    "    food_trimmed[\"P344pr\"], food_trimmed[\"P550tpr\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the first number is the correlation coefficient and the second number is its associated $p$ value.\n",
    "\n",
    "The correlation is positive so as income goes up, expenditure on food goes up (if it were negative it would be a negative correlation, which would state that as income went up expenditure on food went down for some reason).\n",
    "The value of 0.63 suggests quite a lot of the variance in expenditure is accounted for by income (so the correlation is strong).\n",
    "\n",
    "The $p$ value is $<< 0.01$ ($<<$ means 'much less than') so it is highly improbable we would see a correlation this large by chance alone, so we have strong evidence to reject the null hypothesis and conclude that there is an association between income and expenditure on food.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "Pearson's correlation coefficient assumes that both variables are numeric and normally distributed for the $p$ value to be accurate.\n",
    "In this case our variables are numeric (income and expenditure) so this assumption is met.\n",
    "\n",
    "Neither variable should have any outliers (defined as any value greater than the mean + 3.29 standard deviations).\n",
    "For income this is ok:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\n",
    "    food_trimmed[food_trimmed.P344pr > \n",
    "                 food_trimmed.P344pr.mean() + (3.29 * food_trimmed.P344pr.std())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are a few outliers for the expenditure variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\n",
    "    food_trimmed[food_trimmed.P550tpr > \n",
    "                 food_trimmed.P550tpr.mean() + (3.29 * food_trimmed.P550tpr.std())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be safe, let's remove these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_trimmed = food_trimmed[food_trimmed.P550tpr < food_trimmed.P550tpr.mean() + (3.29 * food_trimmed.P550tpr.std())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatterplot of these two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_trimmed.plot.scatter(\"P344pr\", \"P550tpr\")\n",
    "plt.xlabel(\"Income\")\n",
    "plt.ylabel(\"Expenditure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points should be linear (i.e. a straight line) and roughly cylindrical to meet the assumptions.\n",
    "If it's too conincal it means the deviances aren't consistent (heteroskedasticity).\n",
    "\n",
    "If these assumptions aren't true of our data we can use **Spearman's $\\rho$** (pronounced 'row').\n",
    "Spearman's $\\rho$ is also useful when we have a numeric variable and an ordinal variable (something we couldn't test with Pearson's $r$).\n",
    "\n",
    "This is a **non--parametric** test.\n",
    "Non--parametric tests tend to be more robust (which is why we can use them when we violate some of the assumptions of the parametric equivalents, in this case Pearon's $r$) but sometimes have lower statistical power.\n",
    "Therefore, try to use the parametric version by default and switch to the non--parametric version when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.spearmanr(\n",
    "    food_trimmed[\"P344pr\"], food_trimmed[\"P550tpr\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this example the correlation statistic is very similar and the $p$ value is still significant ($<< 0.01$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
